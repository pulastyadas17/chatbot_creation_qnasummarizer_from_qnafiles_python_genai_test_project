# -*- coding: utf-8 -*-
"""Chatbot_Creation_Summarization_from_qnafiles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cdWxQK3k-vjVuOTU94LCZGIFrnF9t5fA
"""

from google.colab import files
import os

# Install necessary packages
!pip install -q pdfplumber openpyxl transformers sentence-transformers

import pdfplumber
import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load semantic model (force it to use CPU)
model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')

# Text extraction function
def extract_text(file_path):
    ext = file_path.split('.')[-1].lower()
    text = ""
    if ext == "pdf":
        with pdfplumber.open(file_path) as pdf:
            text = "\n".join([page.extract_text() for page in pdf.pages if page.extract_text()])
    elif ext == "txt":
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
    elif ext in ["xls", "xlsx"]:
        df = pd.read_excel(file_path)
        text = df.astype(str).apply(lambda x: ' '.join(x), axis=1).str.cat(sep='\n')
    return text

# Semantic search-based response
def find_best_answer(user_question, document_text):
    if not document_text:
        return "Sorry, I couldn't extract any text from the document."

    # Split the document into sentences (or paragraphs)
    document_sentences = document_text.split('\n')

    # Encode the document text and user question
    document_embeddings = model.encode(document_sentences, convert_to_tensor=True, device='cpu')
    query_embedding = model.encode(user_question, convert_to_tensor=True, device='cpu')

    # Calculate cosine similarity
    scores = util.pytorch_cos_sim(query_embedding, document_embeddings)[0]

    # Find the best matching sentence/paragraph
    best_match_idx = int(scores.argmax())
    best_score = float(scores[best_match_idx])

    # Return the best match if score is above a threshold
    if best_score > 0.5:
        return document_sentences[best_match_idx]
    else:
        return "Sorry, I couldn't find a relevant answer."

# Function to save unanswered questions to a file
def save_unanswered_question(question):
    with open('unanswered_questions.txt', 'a') as file:
        file.write(f"{question}\n")

# Main function
def chat_with_document():
    print("\nâœ… File processed successfully!\n")
    print("ðŸ“Œ You can ask questions based on the uploaded document. Type 'exit' to end the chat.\n")

    while True:
        user_query = input("Enter your question (or type 'exit'): ")

        # Graceful exit condition
        if user_query.lower().strip() in ['exit', 'bye', 'quit']:
            print("ðŸ’¬ Answer: Thank you for reaching out! Have a great day ðŸ˜Š")
            break

        # Get the answer based on the query
        answer = find_best_answer(user_query, document_text)

        # If no relevant answer is found, save the unanswered question
        if answer == "Sorry, I couldn't find a relevant answer.":
            save_unanswered_question(user_query)

        print("\nðŸ’¬ Answer:", answer)
        print("\nYou can ask again or type 'exit' to end the chat.")

# Load the uploaded file
uploaded = files.upload()
file_path = next(iter(uploaded))

# Extract text from the uploaded file
document_text = extract_text(file_path)

# Run the function
if __name__ == '__main__':
    chat_with_document()

# Display contents of unanswered_questions.txt
with open('unanswered_questions.txt', 'r') as f:
    print(f.read())

